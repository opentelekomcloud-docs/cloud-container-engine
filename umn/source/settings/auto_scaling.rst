:original_name: cce_10_0786.html

.. _cce_10_0786:

Auto Scaling
============

Auto Scale-Out Settings
-----------------------

CCE Cluster Autoscaler comprehensively checks the resource statuses of an entire cluster. When the load of a microservice is high (for example, the CPU or memory usage is too high), it will add more pods to reduce the load.

**Node Scale-Out Criteria**

-  **Auto Node Scale-Out**: If a workload pod cannot be scheduled, the system automatically scales out the node pool with auto scaling enabled. If the pod has been configured to be affinity for a node, the system will not automatically add more nodes.

   Such auto scaling works with an HPA policy. For details, see :ref:`Using HPA and CA for Auto Scaling of Workloads and Nodes <cce_10_0300>`.

-  **Custom Node Scale-Out**: specifies whether to automatically scale out a node pool based on the :ref:`node scaling policies <cce_10_0209>`. This function is enabled by default.

.. note::

   When both **Auto Node Scale-Out** and :ref:`metric-based node scaling policies <cce_10_0209>` are enabled and their conditions are met, CCE prioritizes the former.

   -  If the scale-out succeeds, CCE skips the metric-based policy and waits for the next cycle.
   -  If the scale-out fails, CCE executes the metric-based policy.

**Upper Resource Limit for Node Scale-Out**

-  **Nodes**: specifies how many nodes can be present in a cluster during scale-out. If there are more nodes in the cluster than the specified value, the cluster will not add nodes. The default value is determined by how many nodes a cluster can manage at most.
-  **CPU Cores**: specifies how many cores on all nodes can be present in a cluster during scale-out. If there are more cores in the cluster than the specified value, the cluster will not add nodes. By default, the number is not limited.
-  **Memory** (GiB): specifies the upper limit of the total memory of all nodes in a cluster during scale-out. If the total memory exceeds the specified value, the cluster will not add nodes. By default, the number is not limited.

.. note::

   When the total number of nodes, CPU cores, and memory in a cluster is collected, unavailable nodes are included in the calculation. However, if a node becomes unavailable due to being deleted from the **default node pool** using *kubectl delete node*, it is excluded from the resource totals.

   This deletion method is not officially supported by CCE. For standard procedures on node deletion, see :ref:`Deleting a Node <cce_10_0186>`.

**Scale-Out Priority**

You can adjust the scale-out priority by dragging the node pool list.

Auto Scale-In Settings
----------------------

CCE Cluster Autoscaler comprehensively checks the resource statuses of an entire cluster. Once it confirms that workload pods can be scheduled and run properly, it automatically obtains nodes for scale-in. Auto scale-in is disabled by default. If you uninstall the add-on and then install it again, you need to enable auto scale-in again.

**Node Scale-In Criteria**

Nodes in a cluster comply with the default scale-in conditions by default. If scale-in conditions are customized for a node pool, the nodes in the node pool comply with the customized scale-in conditions.

.. table:: **Table 1** Node scale-in conditions

   +---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
   | Parameter                             | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
   +=======================================+=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================+
   | Default Scale-In Conditions           | If the CPU and memory allocation rates of a node are lower than a certain percentage (50% by default) for a period of time (10 minutes by default), or the node is unavailable for a period of time (20 minutes by default), the node will be scaled in.                                                                                                                                                                                                                                                                                                                                                                                                    |
   |                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
   |                                       | **Allocation rate = Total requested resources of all pods/Allocatable resources on the node**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
   |                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
   |                                       | If the option **Exclude CPU and memory resources pre-allocated to DaemonSet pods** is selected, CCE will not consider the CPU and memory resources pre-allocated to DaemonSet pods when determining whether to scale in cluster nodes. This means that the resources used by DaemonSet pods will not affect the scale-in decision. If this option is not selected, the resources pre-allocated to DaemonSet pods will be included in the resource allocation calculations. This can cause the CPU and memory allocation rates to exceed the node scale-in threshold, potentially preventing nodes with low CPU and memory utilization from being scaled in. |
   +---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
   | (Optional) Custom Scale-In Conditions | You can configure scale-in conditions for each node pool. If the CPU and memory allocation rates of nodes in a node pool are lower than a certain percentage (50% by default) for a period of time (10 minutes by default), the node pool will be scaled in.                                                                                                                                                                                                                                                                                                                                                                                                |
   |                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
   |                                       | Custom scale-in conditions are supported when the CCE Cluster Autoscaler add-on version is v1.25.181, v1.27.152, v1.28.120, v1.29.81, v1.30.48, v1.31.10, or later. If the auto scaling function is not enabled for all flavors in a node pool, custom scale-in conditions configured for the node pool do not take effect. For details about how to enable the auto scaling function for a node pool, see :ref:`Configuring a Node Pool Auto Scaling Policy <cce_10_0209__section4444195220142>`.                                                                                                                                                          |
   +---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
   | Scale-In Exception Scenarios          | When a node meets the following exception scenarios, CCE will not scale in the node even if the node resources or status meets scale-in conditions:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
   |                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
   |                                       | -  Resources on other nodes in the cluster are insufficient.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
   |                                       | -  Scale-in protection is enabled on the node. To enable or disable node scale-in protection, choose **Nodes** in the navigation pane and then click the **Nodes** tab. Locate the target node and choose **More** > **Enable Scale-in Protection** or **Disable Scale-in Protection** in the **Operation** column.                                                                                                                                                                                                                                                                                                                                         |
   |                                       | -  There is a pod with the non-scale label on the node.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
   |                                       | -  Policies such as reliability have been configured on some containers on the node.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
   |                                       | -  There are non-DaemonSet pods in the **kube-system** namespace on the node.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
   |                                       | -  (Optional) A container managed by a third-party pod controller is running on a node. Third-party pod controllers are for custom workloads except Kubernetes-native workloads such as Deployments and StatefulSets. Such controllers can be created using `CustomResourceDefinitions <https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions>`__.                                                                                                                                                                                                                                                |
   +---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

**Node Scale-In Policy**

.. table:: **Table 2** Node scale-in policy configurations

   +------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+
   | Item                         | Description                                                                                                                                                                                                                                                                             | Default Value         |
   +==============================+=========================================================================================================================================================================================================================================================================================+=======================+
   | Max Nodes for Batch Deletion | Maximum number of idle nodes that can be deleted concurrently.                                                                                                                                                                                                                          | 10                    |
   |                              |                                                                                                                                                                                                                                                                                         |                       |
   |                              | Only idle nodes can be concurrently scaled in. Nodes that are not idle can only be scaled in one by one.                                                                                                                                                                                |                       |
   |                              |                                                                                                                                                                                                                                                                                         |                       |
   |                              | .. note::                                                                                                                                                                                                                                                                               |                       |
   |                              |                                                                                                                                                                                                                                                                                         |                       |
   |                              |    During a node scale-in, if the pods on the node do not need to be evicted (such as DaemonSet pods), the node is idle. Otherwise, the node is not idle.                                                                                                                               |                       |
   +------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+
   | Check Period                 | Interval at which a node can be checked again after it is determined that the node cannot be scaled in                                                                                                                                                                                  | 5 minutes             |
   +------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+
   | Cooldown Duration            | The length of time before CCE starts evaluating scale-in again after an auto scale-in in a cluster.                                                                                                                                                                                     | 10 minutes            |
   |                              |                                                                                                                                                                                                                                                                                         |                       |
   |                              | .. note::                                                                                                                                                                                                                                                                               |                       |
   |                              |                                                                                                                                                                                                                                                                                         |                       |
   |                              |    If both auto scale-out and scale-in exist in a cluster, set this parameter to 0 minutes. This prevents the node scale-in from being blocked due to continuous scale-out of some node pools or retries upon a scale-out failure, which results in unexpected waste of node resources. |                       |
   +------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+
   |                              | The length of time before CCE starts evaluating scale-in again after an auto scale-out in a cluster.                                                                                                                                                                                    | 10 minutes            |
   +------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+
   |                              | The length of time before CCE starts evaluating scale-in again after an auto scale-in failure in a cluster.                                                                                                                                                                             | 3 minutes             |
   +------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+
